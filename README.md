# Begonia_Series / “海棠”系列丛书

在看目前论文的过程中我经常发现很多基本概念不是很清楚，而论文是不会详细介绍这些概念的，这就导致我很多时候看论文看的很糊涂。因此该系列的书籍（或者说文档）将聚焦于人工智能相关基本概念的阐述，内容多来源于我日常的笔记，希望对大家能有所帮助。

## Mathematical Principles of Neural Computing / 神经计算的数学原理

该书籍计划详细阐述神经计算的数学原理，目的在于尽可能地给出详细而清晰的推导以及渐进式的学习曲线。书中只会涉及一些核心概念，防止增加新手读者的困惑。计划的目录如下：

- 1. Linear Regression
  - 1.1. Linear Model
  - 1.2. Performance Measure
  - 1.3. Algorithm Implementation
- 2. Polynomial Regression
- 3. Regularization
  - 3.1. Regularized Least Square Regression
- 4. Gradient Descent(GD, 梯度下降)
  - 4.1. Gradient Descent for Linear Regression
    - 4.1.1. Algorithm Implementation
  - 4.2. Stochastic Gradient Descent
  - 4.3. Minibatch SGD
    - 4.3.1. Algorithm Implementation
  - 4.4. Linear Models for Classification
  - 4.5. SGD for Linear Classification Models
    - 4.5.1. Minibatch SGD Algorithm
- 5. Perceptron and Neural Network
  - 5.1. Perceptron
  - 5.2. Single-Layer Neural Network
    - 5.2.1. Chain Rule and Gradient Descent for Single-Layer Neural Network
    - 5.2.2. Univariate Chain Rule
    - 5.2.3. Multivariate Chain Rule
    - 5.2.4. Gradient Descent for Single-Layer NN
  - 5.3. Feed-Forward Neural Networks
    - 5.3.1. Weights and Bias
    - 5.3.2. Vectorization
    - 5.3.3. Forward Propagation to Compute Prediction
    - 5.3.4. Training MLPs
- 6. Backpropagation Algorithm
  - 6.1. Chain Rule in Computation Graph
  - 6.2. Backpropagation Algorithm in MLPs
  - 6.3. Recursive Relationship on Backpropagated Gradients
  - 6.4. Backpropagation Algorithm
  - 6.5. Gradient Descent for Feedfoward Networks
  - 6.6. Summary

## Introduction to Reinforcement Learning / 强化学习导论

该文档的内容主要来自[赵世钰](https://www.shiyuzhao.net/) 老师的强化学习[书籍及课程](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning?tab=readme-ov-file)的笔记，非常建议大家移步老师的网络课程进行学习。